# ã€10ã€‘è³‡å®‰çš„åŸç½ª ch.3-1.a å–®æ©Ÿé›»è…¦

## ğŸ–¥ï¸ 1. Standalone Computers (1940s)

The first true electronic computer, **ENIAC (Electronic Numerical Integrator and Computer)**, was developed in the 1940s and marked a pivotal moment in computing history. It was a massive, room-sized machine capable of performing thousands of calculations per second.

At the time, users had to punch their programs onto physical cards and submit entire **batches** of these cards to system operators for processing â€” a slow and non-interactive experience.

---

## ğŸ§  2. The Stored-Program Concept (1945)

In 1945, **John von Neumann** proposed the **stored-program concept** in his report *â€œFirst Draft of a Report on the EDVAC.â€* This model became the foundation of modern computing.

Key ideas:

* Both **program instructions** and **data** are stored in the same memory.
* The CPU fetches instructions just like it fetches data.
* Computers could be **reprogrammed** by loading new code into memory, removing the need for rewiring.

This innovation dramatically increased flexibility and paved the way for interactive computing.

---

## â±ï¸ 3. Time-Sharing and CTSS (1961)

The **Compatible Time-Sharing System (CTSS)** was developed at MIT in 1961. It was one of the first operating systems that allowed **multiple users** to access a single computer **simultaneously** by dividing CPU time among users â€” a concept known as **time-sharing**.

This shift:

* Replaced slow batch processing.
* Enabled **interactive computing** through user terminals.
* Improved system resource utilization and user experience.

---

TODO:
## ğŸ” 4. Formal Models of Security

### 4.1 The Bellâ€“LaPadula Model & the Basic Security Theorem (1970s)

Developed by **David Elliott Bell** and **Leonard J. LaPadula** for the U.S. Department of Defense, the **Bellâ€“LaPadula model** is a formal **access control model** focusing on **confidentiality**.

#### Core Concepts:

* **Subjects**: users, processes
* **Objects**: files, databases
* **Security levels**: Unclassified, Confidential, Secret, Top Secret

#### Key Rules:

* **Simple Security Property (No Read Up)**: Users canâ€™t read data above their clearance level.
* \***-Property (No Write Down)**: Users canâ€™t write data to a lower clearance level.

TODO:
#### ğŸ”’ Basic Security Theorem:

> **If a system starts in a secure state and all transitions preserve the security properties, the system remains secure.**

Mathematically:
**Secure Initial State + Secure Transitions â‡’ Always Secure**

TODO:
### 4.2 Formal Verification

Introduced formally in the **1983 U.S. Department of Defense â€œOrange Bookâ€**, also known as the **Trusted Computer System Evaluation Criteria (TCSEC)**.

**Formal verification** is a mathematical method to prove that systems behave according to their specifications â€” especially in critical applications like military, medical, and aerospace systems.

---

## é›£ä»¥è§£æ±ºçš„å•é¡Œ

### 5.1 Shared Memory for Code & Data: A Double-Edged Sword

The innovation of sharing memory between **code and data** increased efficiency but also introduced a major vulnerability:

> If the system cannot distinguish between â€œdata to be processedâ€ and â€œcode to be executed,â€ malicious input can be **executed as code** â€” the foundation of many modern attacks.

To fix this would require a fundamental shift â€” such as **hardware-level separation** of code and data â€” which is rarely practical.

---

### 5.2 Covert Channels & the Confinement Problem
ç¡¬é«”ç‰©ç†çš„é‹ä½œéµå¾ªç‰©ç†çš„æ³•å‰‡ï¼Œéš±å¯†éš§é“çš„å•é¡Œå°‡æ°¸é å­˜åœ¨ï¼Œè€Œç¾åœ¨éš¨è‘—è¡Œå‹•é‹ç®—ã€é›²ç«¯é‹ç®—çš„ç››è¡Œï¼Œé€™å°‡æœƒæ˜¯æœªä¾†ææ€•ä¸å¾·ä¸åœ¨é¢å°çš„å•é¡Œ
TODO: A49
**Covert channels** are unintended communication paths that bypass normal security controls â€” often leveraging hardware behavior.

* **Confinement Problem**: Itâ€™s impossible to guarantee that a program cannot leak data to the outside world.
* These channels **donâ€™t rely on software vulnerabilities** and are difficult to detect or eliminate.

TODO:
As mobile and cloud computing become dominant, these threats become more serious.

---


### 5.3 Operating System Trust & Compiler Backdoors

TODO: Multicsç·¨è­¯å™¨å•é¡Œ æ¹¯æ™®æ£®ä¸å¾—ä¸æˆè¦ªæ²’æœ‰å°‡å¾Œé–€å°å…¥UNIX
All software must ultimately run on an **operating system**, which itself must be compiled â€” and **compilers can be subverted**.

* **Ken Thompson** (creator of UNIX) demonstrated in his famous talk *â€œReflections on Trusting Trustâ€* how a compiler could insert a **backdoor** into any program it compiles â€” even into itself â€” in a way that is **invisible** to code reviewers.
* Unless you write **everything from scratch** (including your compiler), you can never fully trust your system.
* Real-world example: **Eric Allmanâ€™s `sendmail`** accidentally left in a backdoor that was later exploited by the **Morris Worm**.

---
TODO:
### 5.4 The Cost of Verified Security

While **mathematically verified security** is ideal, it is:

* **Time-consuming**
* **Expensive**
* Often **commercially unviable**

Example:
The **SCOMP computer**, one of the few systems to achieve **A1-level certification** (highest under TCSEC), sold fewer than **30 units** worldwide.
Meanwhile, only around **200 experts** globally are capable of evaluating software to that standard.


---

ä»»ä½•å®‰å…¨æ€§åŠªåŠ›å¿½è¦–äº†ä½œæ¥­ç³»çµ±çš„å®‰å…¨æ€§ï¼Œé‚£å°±ç„¡ç•°æ–¼æŠŠå ¡å£˜å»ºåœ¨æ²™å­ä¸Šã€‚

"The Inevitable Failure" (often referenced as "The Inevitability of Failure") is a seminal paper by P. Loscocco et al. published in 1998 that argues failure is a normal, unavoidable characteristic of complex computing systems


[^1]: https://ieeexplore.ieee.org/document/601735