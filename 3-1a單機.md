# 【10】資安的原罪 ch.3-1.a 單機電腦

本章探討電腦從誕生到現代電腦的雛形，並介紹早期資訊安全概念的起源與挑戰。

---

## 電腦發展過程的關鍵奠基石

### 獨立電腦 (1940年代)

第一台真正的電子電腦 **ENIAC (Electronic Numerical Integrator and Computer)** 於1940年代誕生，標誌計算歷史上的重要里程碑。該機器龐大，占據整個房間，能每秒執行數千次運算。

當時，使用者需將程式打孔於實體卡片上，並將整批卡片交由系統操作員處理，過程緩慢且不具互動性。程式修改必須透過重新接線硬體完成。

### 存儲程式概念 (1945年)

1945年，*John von Neumann* 在其報告《First Draft of a Report on the EDVAC》中提出了 **存儲程式概念（stored-program concept）**，成為現代計算的基礎。

核心概念包括：

* 程式指令與數據同時存放於記憶體中。
* CPU 透過抓取記憶體中的指令進行執行。
* 電腦可透過載入新程式碼來重新編程，不再依賴硬體重新接線。

此創新大幅提升靈活性，為互動式計算奠定基礎。

### 編譯器（1950年代）

編譯器（Compiler）是一種將**高階程式語言翻譯成機器語言**的軟體工具。此過程對現代電腦運作至關重要，使人類能以易懂語言編寫程式，電腦則能理解並執行。

### 分時系統 (1961年)

分時系統（Compatible Time-Sharing System，CTSS）於1961年由麻省理工學院（MIT）開發，是首批允許**多使用者同時訪問單一電腦的作業系統之一**。透過將 CPU 時間分配給不同使用者，此系統實現了時間共享（time-sharing）。

此創新取代了緩慢的批次處理，使互動計算成為可能，提升系統資源利用率與使用者體驗。

### UNIX (1969)

UNIX 是由 *Ken Thompson* 與 *Dennis Ritchie* 於貝爾實驗室（Bell Labs）開發的作業系統。其設計哲學重視**簡潔性**與**模組化**，透過小型工具實現強大功能，並支援多使用者與多任務處理。

UNIX 的誕生深刻改變電腦使用方式，成為現代作業系統（如 Linux 與 macOS）的基礎。

---

## 資訊安全的發展嘗試

隨著電腦發展，安全性議題逐漸浮現。以下介紹早期學者對資安的研究嘗試。

### Bell–LaPadula 模型與基本安全定理 (1970年代)

由 *David Elliott Bell* 與 *Leonard J. LaPadula* 為美國國防部開發的 **Bell–LaPadula 模型**，是一種專注於 **機密性 (confidentiality)** 的正式訪問控制模型。

#### 核心概念：

* 主體 (Subjects)：使用者、進程
* 對象 (Objects)：檔案、資料庫
* 安全等級 (Security Levels)：未分類、機密、秘密、絕密

#### 主要規則：

* **簡單安全性屬性（Simple Security Property）**：使用者無法讀取高於其安全等級的數據（無法向上讀取）。
* **\*屬性（Star Security Property）**：使用者無法將數據寫入低於其安全等級的區域（無法向下寫入）。

#### 基本安全定理：

> **若系統以安全狀態啟動，且所有狀態轉換均遵守安全規則，則系統將保持安全。**

### 形式驗證 (1980年代)

1983年，美國國防部於**橘皮書**（TCSEC，Trusted Computer System Evaluation Criteria）中正式介紹了**形式驗證（Formal Verification）**。

形式驗證是一種以數學方法檢驗系統是否符合規範的技術，常用於軍事、醫療與航空等對安全性要求極高的領域。

#### 形式驗證運作步驟：

1. **規範定義**：明確系統應具備的屬性，如存取控制政策、資訊不外洩等。
2. **建模**：建立系統正式模型，常用形式邏輯或有限狀態機方法。
3. **證明**：利用模型檢查（Model Checking）或定理證明（Theorem Proving）等數學技術驗證系統符合規範。
4. **驗證**：確認模型行為與系統實際運作一致。

---

## 「原罪」

### 1. 程式碼與資料共享記憶體
此設計提升系統效率，卻埋下資安隱患。當系統無法嚴格**區分資料與程式碼時**，攻擊者可能將惡意資料注入記憶體，使其在執行階段被錯誤當成程式碼執行，導致安全漏洞，成為現代攻擊常見手法。

### 2. 多使用者共用同一系統

隔離問題（Confinement Problem）是關於我們如何確保電腦上的資料不會遭到洩漏。而隱蔽通道（Covert channels）指的是那些非預期、隱藏的通訊路徑。其中一種方法便是利用電腦硬體的基本特性——如電子流動、電力消耗、熱量變化與時間差異等——來傳遞資訊。 由於此種通道依賴的是物理定律，只要我們的**系統仍運行在現實的硬體上**，隱蔽通道問題就無法被徹底根除。

隨著行動裝置、雲端平台及共享硬體普及，隱蔽通道問題也愈發重要。

### 3. 軟體後門

所有軟體均有可能藏有後門，甚至編譯器本身也可能成為後門的來源。

1984年，UNIX 共同創作者 *Ken Thompson* 在著名演講《信任的反思（Reflections on Trusting Trust）》中展示驚人概念：植入惡意程式碼的編譯器能在不被察覺情況下，自動在所有被編譯的程式中插入後門，即使原始碼完全乾淨。

此概念顯示，除非從**零開始建立所有程式碼、編譯器與系統元件**，否則任何系統都無法信任。

### 4. 驗證安全的成本

雖然數學驗證理論上可達到高度安全，但過程耗時且昂貴，**商業上難以廣泛應用**。舉例來說，SCOMP 電腦是少數獲得 A1 級認證（TCSEC最高級別）的系統之一，全球銷量不足30台。

此外，具備形式驗證專業能力的專家全球僅約200位。隨著電腦功能日益複雜，數學形式安全驗證的難度也逐漸提高。



[^1]: https://ieeexplore.ieee.org/document/601735